#!/usr/bin/env python3
import argparse
import collections
import difflib
import glob
import gzip
import os
import pathlib
import re
import shutil
import subprocess
import sys
import time
from unidecode import unidecode
import urllib.request
from xml.etree import ElementTree
import yaml

import bibtexparser
from bibtexparser import bparser as bp
from pylatexenc import latexwalker as lw
from pylatexenc import latexencode as le

COMMANDS = ['setup', 'add', 'update', 'pull-acl', 'pull-arxiv', 'list']
ERROR_MESSAGE = 'Usage: {} [{}] <command flags>'.format(
        sys.argv[0], '|'.join(sorted(COMMANDS)))
CITE_MACROS = ['cite', 'citep', 'citet', 'citealp', 'citep*', 'citet*', 'citeauthor', 'citeyear']
KEYS_TO_DELETE = ['address', 'doi', 'isbn', 'keywords', 'location', 'month', 'number', 'numpages', 'pages', 'series', 'volume']
NON_BOOK_KEYS_TO_DELETE = ['editor', 'publisher']
BIBTEX_ID_REGEX = re.compile(r'@[^{]*\{(\S*),')

# Saved nltk stopwords list because importing nltk is so slow
STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

# Files
ROOT_DIR = pathlib.Path(__file__).resolve().parent
PROJECTS_FILE = os.path.join(ROOT_DIR, 'projects.yml')
ASSETS_DIR = os.path.join(ROOT_DIR, 'assets')
OVERLEAF_DIR = os.path.join(ROOT_DIR, 'overleaf')
CUSTOM_BIB_PATH = os.path.join(ROOT_DIR, 'custom.bib')
BIBTEX_NAME = 'refs.bib'

# ACL Anthology
ACL_ANTHOLOGY_URL = 'https://aclanthology.org/anthology.bib.gz'
ACL_ANTHOLOGY_GZ_PATH = os.path.join(ASSETS_DIR, 'anthology.bib.gz')
ACL_ANTHOLOGY_PATH = os.path.join(ASSETS_DIR, 'anthology.bib')

# arXiv
ARXIV_REGEX = r'/[a-z]+/([0-9]+\.[0-9]+)([^0-9].*)?'
ARXIV_IDS_PATH = os.path.join(ROOT_DIR, 'arxiv_ids.txt')
ARXIV_BIB_PATH = os.path.join(ASSETS_DIR, 'arxiv.bib')


# booktitle shortening  
CONFERENCE_SHORT_NAMES = [
        (re.compile(r'Proceedings of the [0-9a-z]+ Annual Meeting of the Association for Computational Linguistics.*'), 'Association for Computational Linguistics'),
        (re.compile(r'Proceedings of the [0-9a-z]+ Conference on Empirical Methods in Natural Language Processing.*'), 'Empirical Methods in Natural Language Processing'),
        (re.compile(r'Proceedings of the [0-9a-z]+ Conference of the North American Chapter of the Association for Computational Linguistics.*'), 'North American Association for Computational Linguistics'),
]
FINDINGS_PATTERN = re.compile('Findings of the Association for Computational Linguistics: ([-A-Z]+) ([0-9]+)')
PROCEEDINGS_PATTERN = re.compile('Proceedings of the ([0-9]+(st|nd|rd|th) )?(.*)')

OPTS = None

def parse_args():
    if len(sys.argv) == 1:
        print(ERROR_MESSAGE, file=sys.stderr)
        sys.exit(1)
    command = sys.argv[1]
    if command not in COMMANDS:
        print(ERROR_MESSAGE, file=sys.stderr)
        sys.exit(1)
    new_argv = sys.argv[2:]
    parser = argparse.ArgumentParser(f'{sys.argv[0]} {command}')
    if command == 'add':
        parser.add_argument('name', help='Name of project')
        parser.add_argument('overleaf_id', help='Overleaf ID of project')
    elif command == 'update':
        parser.add_argument('name', help='Name of project')
        parser.add_argument('--force', '-f', action='store_true', 
                            help='Push .bib even if not all refs are found') 
        parser.add_argument('--no-url', action='store_true',
                            help='Delete url field from BibTeX entries')
        parser.add_argument('--shorten-booktitle', '-s', action='store_true',
                            help='Shorten booktitle fields')
        parser.add_argument('--dry-run', '-d', action='store_true',
                            help='Do not pull or push from Overleaf')
        parser.add_argument('--fast', action='store_true',
                            help='Run in fast mode (no checking arXiv)') 
    if command in ('add', 'update') and len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)
    return command, parser.parse_args(new_argv)

def load_projects():
    with open(PROJECTS_FILE) as f:
        projects = yaml.safe_load(f)
    if not projects:
        return []
    else:
        return projects

def run_setup():
    # Set up file structure
    print('Creating file structure...')
    pathlib.Path(PROJECTS_FILE).touch()
    pathlib.Path(ASSETS_DIR).mkdir(exist_ok=True)
    pathlib.Path(OVERLEAF_DIR).mkdir(exist_ok=True)

    # Pull ACL Anthology
    run_pull_acl()

    print('Setup complete.')

def run_add():
    print('Reminder: Use username "git" and password from Overleaf Git authentication token')
    # Check for duplicates
    projects = load_projects()
    for p in projects:
        if p['name'] == OPTS.name:
            raise ValueError(f'Project with name {OPTS.name} already exists')
        if p['overleaf_id'] == OPTS.overleaf_id:
            raise ValueError(f'Project with ID {OPTS.overleaf_id} already exists')

    # Clone the Overleaf git repository
    os.chdir(OVERLEAF_DIR)
    subprocess.run(['git', 'clone', f'https://git.overleaf.com/{OPTS.overleaf_id}', OPTS.name], check=True)

    # Save to projects list
    projects.append({'name': OPTS.name, 'overleaf_id': OPTS.overleaf_id})
    with open(PROJECTS_FILE, 'w') as f:
        yaml.dump(projects, f)

def get_project(name):
    projects = load_projects()
    for p in projects:
        if p['name'] == name:
            return (p['name'], p['overleaf_id'])
    raise KeyError(f'No project with name {name}')

def _find_refs_from_node(refs, filename, node):
    if node.isNodeType(lw.LatexMacroNode) and node.macroname in CITE_MACROS:
        # Found a \cite node
        for arg in node.nodeargd.argnlist:
            if arg and arg.isNodeType(lw.LatexGroupNode):
                if not arg.nodelist:
                    continue # Ignore things like \citep{}
                char_node = arg.nodelist[0]
                assert char_node.isNodeType(lw.LatexCharsNode)
                cur_refs = [x.strip() for x in char_node.chars.split(',')]
                for r in cur_refs:
                    refs[r].add(filename)
    if node.isNodeType(lw.LatexMacroNode) or node.isNodeType(lw.LatexEnvironmentNode):
        # Recursive case for arguments
        if node.nodeargd:
            for arg in node.nodeargd.argnlist:
                if arg:
                    _find_refs_from_node(refs, filename, arg)
    if node.isNodeType(lw.LatexGroupNode) or node.isNodeType(lw.LatexEnvironmentNode):
        # Recursive case for content within environments/groups
        for arg in node.nodelist:
            if arg:
                _find_refs_from_node(refs, filename, arg)

def find_refs(project_dir):
    refs = collections.defaultdict(set)
    for filename in glob.glob(os.path.join(
            project_dir, '**', '*.tex'), recursive=True):
        print(f'Processing {filename}...')
        relative_filename = pathlib.Path(filename).relative_to(project_dir).name
        with open(filename) as f:
            tex = f.read()
        w = lw.LatexWalker(tex)
        nodelist, pos, length = w.get_latex_nodes()
        for node in nodelist:
            _find_refs_from_node(refs, relative_filename, node)

    #print(refs)
    return refs

def shorten_booktitle(title):
    for pattern, name in CONFERENCE_SHORT_NAMES:
        if re.match(pattern, title):
            return name
    m = re.match(FINDINGS_PATTERN, title)
    if m:
        return 'Findings of {}'.format(m.group(1))
    m = re.match(PROCEEDINGS_PATTERN, title)
    if m:
        return m.group(3)
    print(f'Could not shorten title: "{title}"')
    return title

def compress_entry(entry):
    for k in KEYS_TO_DELETE:
        if k in entry:
            del entry[k]
    if entry['ENTRYTYPE'] != 'book':
        for k in NON_BOOK_KEYS_TO_DELETE:
            if k in entry:
                del entry[k]
    if OPTS.no_url and 'url' in entry:
        del entry['url']
    if OPTS.shorten_booktitle and 'booktitle' in entry:
        entry['booktitle'] = shorten_booktitle(entry['booktitle'])

def add_entries(id_to_entry, bib):
    for entry in bib.entries:
        if entry['ID'] in id_to_entry:
            print('Warning: Duplicate BibTeX entry {}'.format(entry['ID']))
        compress_entry(entry)
        id_to_entry[entry['ID']] = entry

def load_bib(bib_file):
    parser = bp.BibTexParser(common_strings=True)
    with open(bib_file) as f:
        return parser.parse(f.read())

def load_bib_with_hints(bib_file, refs, bib_id_set):
    """Load the given .bib file but only keep entries with relevant IDs.
    
    We need this because parsing the whole ACL Anthology file with bibtexparser
    takes too long.

    We're not worried about catching entries that aren't actually relevant,
    as long as we reduce the number of bibtexparser calls to a reasonable 
    number.

    As a side effect, also adds all found ID's from the .bib to bib_id_set.
    """
    parser = bp.BibTexParser(common_strings=True)
    ref_set = set(refs)
    kept_lines = []
    is_relevant = False
    with open(bib_file) as f:
        for line in f:
            if line.startswith('@'):
                m = re.search(BIBTEX_ID_REGEX, line)
                if m:
                    bib_id = m.group(1)
                    bib_id_set.add(bib_id)
                    is_relevant = bib_id in refs
                else:
                    print(f'Warning: Could not parse BibTeX ID from line "{line}"')
                    is_relevant = False
            if is_relevant:
                kept_lines.append(line)
    kept_string = ''.join(kept_lines)
    return parser.parse(kept_string)

def find_bib_entries(refs):
    if not OPTS.fast:
        run_pull_arxiv()
    bib_id_set = set()
    print('Loading custom.bib...')
    custom_bib = load_bib_with_hints(CUSTOM_BIB_PATH, refs, bib_id_set)
    print('Loading ACL Anthology BibTeX...')
    acl_bib = load_bib_with_hints(ACL_ANTHOLOGY_PATH, refs, bib_id_set)
    print('Loading arXiv BibTeX...')
    arxiv_bib = load_bib_with_hints(ARXIV_BIB_PATH, refs, bib_id_set)
    id_to_entry = {}
    add_entries(id_to_entry, acl_bib)
    add_entries(id_to_entry, arxiv_bib)
    add_entries(id_to_entry, custom_bib)

    print('Locating matching entries for references...')
    bib_entries = []
    found_all = True
    for ref in refs:
        if ref in id_to_entry:
            bib_entries.append(id_to_entry[ref])
        else:
            match_str = ''
            close_matches = difflib.get_close_matches(ref, bib_id_set)
            if close_matches:
                match_str = '. Did you mean: {}?'.format(', '.join(close_matches))
            print(f'Warning: No BibTeX entry found for {ref}{match_str}')
            found_all = False
    return bib_entries, found_all

def write_bib(bib_entries, filename):
    db = bibtexparser.bibdatabase.BibDatabase()
    db.entries = sorted(bib_entries, key=lambda x: x['ID'])
    with open(filename, 'w') as f:
        bibtexparser.dump(db, f)

def run_update():
    name, overleaf_id = get_project(OPTS.name)
    project_dir = os.path.join(OVERLEAF_DIR, name)

    # Pull the Overleaf git repository
    if not OPTS.dry_run:
        print('Pulling Overleaf changes...')
        os.chdir(project_dir)
        subprocess.run(['git', 'pull'])

    # Find refs
    refs = find_refs(project_dir)

    # Match citations
    bib_entries, found_all = find_bib_entries(refs)
    if not found_all and not OPTS.force:
        print('Missing refs detected, exiting.')
        sys.exit(1)

    # Write BibTeX
    bibtex_path = os.path.join(project_dir, BIBTEX_NAME)
    write_bib(bib_entries, os.path.join(bibtex_path))

    # Commit and push updates
    if not OPTS.dry_run:
        print('Pushing to Overleaf...')
        subprocess.run(['git', 'add', BIBTEX_NAME])
        subprocess.run(['git', 'commit', '-m', 'Update references'])
        subprocess.run(['git', 'push'])

def run_pull_acl():
    print('Downloading ACL Anthology BibTeX...')
    urllib.request.urlretrieve(ACL_ANTHOLOGY_URL, ACL_ANTHOLOGY_GZ_PATH)
    with gzip.open(ACL_ANTHOLOGY_GZ_PATH) as f_in:
        with open(ACL_ANTHOLOGY_PATH, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

def run_pull_arxiv():
    print('Reading arXiv cache...')
    arxiv_id_to_entry = {}
    if os.path.isfile(ARXIV_BIB_PATH):
        bib = load_bib(ARXIV_BIB_PATH)
        for entry in bib.entries:
            arxiv_id_to_entry[entry['eprint']] = entry

    # Find which ID's are new
    new_ids = []
    arxiv_ids = []
    with open(ARXIV_IDS_PATH) as f:
        # Allow comments using '#'
        for line in f:
            a = line.split('#')[0].strip()
            if a:
                arxiv_ids.append(a)
    for arxiv_id in arxiv_ids:
        if arxiv_id not in arxiv_id_to_entry:
            new_ids.append(arxiv_id)
    if not new_ids:
        print('No new arXiv IDs.')
        return

    print(f'Downloading arXiv metadata for {len(new_ids)} articles...')
    with urllib.request.urlopen('http://export.arxiv.org/api/query?id_list={}'.format(','.join(new_ids))) as url:
        r = url.read()
    tree = ElementTree.fromstring(r)
    for e in tree:
        if e.tag == '{http://www.w3.org/2005/Atom}entry':
            entry = {'archivePrefix': 'arXiv'}
            authors = []
            first_author_name = None
            title_start = None
            for child in e:
                if child.tag == '{http://www.w3.org/2005/Atom}updated':
                    entry['year'] = child.text.split('-')[0]
                elif child.tag == '{http://www.w3.org/2005/Atom}title':
                    entry['title'] = le.unicode_to_latex(child.text)
                    title_words = re.sub(r'[^a-z ]', '', unidecode(child.text).lower()).strip().split()
                    for w in title_words:
                        if w not in STOPWORDS:
                            title_start = w
                            break
                    else:
                        title_start = title_words[0]
                elif child.tag == '{http://www.w3.org/2005/Atom}id':
                    entry['url'] = child.text
                    url = urllib.parse.urlparse(child.text)
                    entry['eprint'] = re.match(ARXIV_REGEX, url.path).group(1)
                elif child.tag == '{http://www.w3.org/2005/Atom}author':
                    for name in child:
                        if name.tag == '{http://www.w3.org/2005/Atom}name':
                            authors.append(le.unicode_to_latex(name.text))
                            if not first_author_name:
                                # Use the last name of the first author
                                first_author_name = re.sub(r'[^a-z ]', '', unidecode(name.text).split(' ')[-1].lower())
            entry['author'] = ' and '.join(authors)
            entry['ID'] = '{}{}{}'.format(first_author_name, entry['year'], title_start)
            entry['ENTRYTYPE'] = 'misc'
            arxiv_id_to_entry[entry['eprint']] = entry

    # Write BibTeX
    print(f'Writing arXiv BibTeX...')
    bib_entries = [arxiv_id_to_entry[x] for x in arxiv_ids]
    write_bib(bib_entries, ARXIV_BIB_PATH)
    return load_bib(ARXIV_BIB_PATH)

def run_list():
    projects = load_projects()
    projects.sort(key=lambda x: x['name'])
    print(f'Available projects ({len(projects)} total):')
    for p in projects:
        print('    * {}: https://www.overleaf.com/project/{}'.format(
                p['name'], p['overleaf_id']))

def main(command):
    if command == 'setup':
        run_setup()
    elif command == 'add':
        run_add()
    elif command == 'update':
        run_update()
    elif command == 'pull-acl':
        run_pull_acl()
    elif command == 'pull-arxiv':
        run_pull_arxiv()
    elif command == 'list':
        run_list()
    else:
        raise ValueError(command)

if __name__ == '__main__':
    command, OPTS = parse_args()
    main(command)
